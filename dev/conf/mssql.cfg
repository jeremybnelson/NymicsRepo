# mssql.cfg

; {database_name}
; {schema_name}
; {table_name}
; {column_definitions}: <column> <type>[(<size> | <precision, scale>)] [null|not null] [default <default>
; {column_names}: column1, column2, ...
; {column_values}: value1, value2, ...
; {column_updates}: column1 = <expression>, ...
; {target_columns_equal_source_columns}: t.column1 = s.column1, ...
; {source_column_names}: s.column1, s.column2, ...
;

[current_timestamp]
select getdate();


[does_database_exist]
select db_id(N'{database_name}');


[create_database]
create database {database_name};

# TODO: SET COMPATIBILITY_LEVEL must be 130 or higher for OPENJSON support
# https://docs.microsoft.com/en-us/sql/t-sql/functions/openjson-transact-sql?view=sql-server-2017
# alter database databasename set compatibility_level = 130;
# https://docs.microsoft.com/en-us/sql/relational-databases/databases/view-or-change-the-compatibility-level-of-a-database


[use_database]
use {database_name};

# TODO: Report on compatiblity_level during logging.
# select compatibility_level from sys.databases where name = '<database-name>';
# https://docs.microsoft.com/en-us/sql/relational-databases/databases/view-or-change-the-compatibility-level-of-a-database


[does_schema_exist]
select * from sys.schemas where name = N'{schema_name}';


[create_schema]
create schema {schema_name};


[does_table_exist]
select object_id(N'{schema_name}.{table_name}', N'u');


[create_table_from_table_schema]
create table {schema_name}.{table_name} (
{column_definitions}
);


[select_table_schema]
select
  column_name,
  data_type,
  is_nullable,
  character_maximum_length,
  numeric_precision,
  numeric_scale,
  datetime_precision,
  character_set_name,
  collation_name
  from information_schema.columns
  where
    table_schema = '{schema_name}' and
    table_name = '{table_name}';


[select_table_pk]
select column_name
  from information_schema.constraint_column_usage
  where
    table_schema = '{schema_name}' and
    table_name = '{table_name}' and
    constraint_name LIKE 'PK%'
  order by column_name;


[select_table]
select {column_names}
  from {schema_name}.{table_name}
  {where_clause}
  {order_by_clause};


[select_table_where_timestamp_condition]
select {source_columns}, {cdc_columns}, {udp_columns}
  from {schema_name}.{table_name} s
  where {timestamp_condition};


[select_table_where_timestamp_condition]
# query for multiple timestamps
# TODO: Use as pattern for multiple rowversions
# TODO: "select max(v) should be generated based on count of timestamps"
[data_capture_query_multi_timestamp]
-- (select ...) expression can be used in where and order by
select
  *,
  (select max(v) from (values (val_1), (val_2), (val_3), (val_4)) as value(v)) as max_value
  from {schema_name}.{table_name} s
  where {timestamp_condition};


[create_temp_table]
create table #{table_name}(
  {column_definitions}
);


[drop_temp_table]
drop table if exists tempdb.dbo.#{table_name};


[insert_into_table]
insert into {schema_name}.{table_name}
  ({column_names})
  values
  ({column_placeholders});


[delete_where]
delete from {schema_name}.{table_name}
  where {}


[update_table]
update {schema_name}.{table_name} set
  {column_updates};


[drop_table]
drop table if exists {schema_name}.{table_name};


[merge_source_to_target]
-- s:source, t:target
merge into {schema_name}.{table_name} as t with (serializable)
  using #{table_name} as s
when matched then
  -- t.column1 = s.column1, ...
  update set
    {target_columns_equal_source_columns}
when not matched then
  insert
    -- (column1, column2, ...)
    ({column_names})
    values
    -- (s.column1, s.column2, ...)
    ({source_column_names});


[get_pk]
select {pk_column_name} as pk
  from {schema_name}.{table_name}
  where {nk_column_name} = {nk_column_value};


; update with CDC template for RTP
[capture_select]
select {column_names}
  from {schema_name}.{table_name};


; -------------------------------
; Specific table definitions
; -------------------------------


[create_named_table_udp_catalog_nst_lookup]
create table udp_catalog.nst_lookup (
  nst_pk int identity(1,1),
  nst_nk nvarchar(512),
  namespace nvarchar(255),
  table_name nvarchar(255)
);


[create_named_table_udp_catalog_job_log]
create table udp_catalog.job_log (
  job_pk int identity(1,1),
  job_nk nvarchar(255),
  job_id int,
  nst_fk int,
  capture_file_size bigint,
  capture_start_time datetime2,
  capture_end_time datetime2,
  archive_start_time datetime2,
  archive_end_time datetime2,
  staging_start_time datetime2,
  staging_end_time datetime2
);


[create_named_table_udp_catalog_stat_log]
create table udp_catalog.stat_log (
  -- session
  script_name nvarchar(32) null,
  script_version nvarchar(16) null,
  script_instance nvarchar(16) null,
  script_project nvarchar(64) null,
  script_stage nvarchar(32) null,
  server_name nvarchar(32) null,
  account_name nvarchar(32) null,
  namespace nvarchar(128) null,

  -- job
  job_id int null,
  stat_name nvarchar(128) null,
  stat_type nvarchar(32) null,
  start_time datetime2 null,
  end_time datetime2 null,
  run_time float,
  row_count bigint,
  data_size bigint
);

# STOPPED: 2018-09-27

[create_named_table_udp_catalog_table_log]
create table udp_catalog.table_log (
  job_fk int,
  nst_fk int,
  table_name nvarchar(127),
  capture_records bigint,
  capture_file_size bigint,
  capture_start_time datetime2,
  capture_end_time datetime2,
  staging_updates bigint,
  staging_inserts bigint,
  staging_start_time datetime2,
  staging_end_time datetime2
);



# tables to insure sequential staging of archived capture files
# using tables vs queues because
# need persistent storage of file names after they are removed from queues
# and we need to process files in job sequential order

[create_named_table_udp_catalog_stage_arrival_queue]
create table udp_catalog.stage_arrival_queue (
  archive_file_name nvarchar(255),
  job_id int,
  queued_timestamp datetime2 default getdate()
);

[create_named_table_udp_catalog_stage_pending_queue]
create table udp_catalog.stage_pending_queue (
  archive_file_name nvarchar(255),
  job_id int,
  queued_timestamp datetime2 default getdate()
);

[select_from_stage_arrival_queue]
-- process oldest arriving files first
select archive_file_name, job_id
  from udp_catalog.stage_arrival_queue
  where archive_file_name in
    (select archive_file_name from udp_catalog.stage_pending_queue)
    or job_id = 1
  order by queued_timestamp;

[delete_from_stage_arrival_queue]
delete from udp_catalog.stage_arrival_queue
  where archive_file_name = {queryparm};

[delete_from_stage_pending_queue]
delete from udp_catalog.stage_pending_queue
  where archive_file_name = {queryparm};


; -------------------------------
; Test resource name value to verify resource file overrides are working
; -------------------------------

[test_resource_name_1]
;
This resource only defined in main (default) resource file.

[test_resource_name_2]

This resource is overriden by /local resource file.

; [test_resource_name_3]
;
; This resource intentionally not defined here.
; This resource should be defined in the /local resource file.
